## Depth from Videos in the Wild:
## Unsupervised Monocular Depth Learning from Unknown Cameras

#### 摘要 

我们提出了一种从単目视频中同时学习深度,自我运动,物体运动和相机内参的新方法,该方法仅使用相邻视频帧之间的一致性作为监督信号.与之前的工作类似,我们的方法通过将differentiable warping应用于视频帧并且将结果与相邻帧进行比较来学习,但是该方法提出了几点改进: 我们直接使用训练中预测的深度图,以几何和可微的方式处理遮挡.我们引入了随机层正则化,一种新的强大的正则器,并且,我们考虑了物体相对于背景的运动.据我们所知,我们的工作是第一个以无监督的方式从视频中学习相机内参,包括镜头畸变.从而允许我们从任意的未知来源的视频中提取精确地深度和运动,我们在cityscapes,KITTI和EuRoc数据集上评估了我们的结果,建立了深度预测和测距的新技术,并定性地证明了深度预测可以从YouTube视频集合中学习.

#### 简介

从视频中估计3D结构和摄像机运动是计算机视觉中的一个关键问题,解决这一问题的传统方法依赖于在多个连续帧中识别场景中的相同点,并求解在这些帧中最大深度一致的3D结构和摄像机运动,但是这样的帧间对应只能建立所有像素的子集,这使得深度估计的问题无法确定.和逆问题一样.  **缺口由连续性和平面性等假设填补**

并非手动制定这些假设,深度学习从数据中获得它们,当信息不足以解决分歧时,深度网络可以从之前见过的例子中通过泛化产生深度图和**flow fields**.无监督方法允许单独从原始视频中学习,使用与传统方法一样的一致性损失,但是在训练中对其进行优化.通过推理,训练好的网络能够从单张图像中预测深度,从成对或者更长的图像序列中预测自我运动.

随着这一方向的研究得到重视,很明显,物体运动是一个主要的障碍,因为他违反了场景是静态的这一假设.已经提出了几个解决这一问题的方向,包括通过实例分割利用场景的语义理解.遮挡是另一个限制因素,最后,在这一方向先前所有的工作中,相机的内参必须给出.

本工作解决了这些问题,因此减少了监督,提高了从无标签视频中预测深度和运动的质量.首先,我们证明了一个深度网络可以被训练来预测相机的内在参数,包括镜头畸变,以一种无监督的方式,从视频本身.其次 ,我们是第一个在这种情况下从预测的深度本身以几何的方式直接处理遮挡.最后,我们大大减少了处理场景中移动元素所需的语义理解量: 并非分割移动对象的每个实例并且跨帧追踪,我们需要一个单一的掩码来覆盖可能属于移动对象的像素,这个掩码可以非常粗糙,实际上可以是矩形边框的组合,获得这样一个掩码是一个非常简单的问题,并且与实例分割相比,使用现有的模型可以更可靠的解决这一问题.

除了这些定性的进步,我们对我们的方法进行了广泛的定量评估,发现他在多个广泛使用的的基准数据集上建立了新的里程碑.将数据集中在一起,这种方法极大地提高了性能.最终,我们第一次证明了从深度和相机内参预测可以在YouTube视频上进行,这些视频是从多个不同的摄像机拍摄的,每一个相机都有未知的并且通常情况下不同的内参.

#### 2 相关工作

场景深度估计是机器人导航和操纵的一项重要任务,历史上对其进行了大量研究,包括大量的立体 多视图几何 主动传感等方面的研究.近年来,基于学习的深度预测方法已经成为基于稠密预测学习的研究热点.其中,场景深度由输入的RGB图像进行预测,深度估计函数由激光雷达等传感器提供监督来学习.类似的方法也适于其他稠密预测,例如表面法线.

##### 无监督深度学习 

深度的无监督学习,唯一的监督来自于单视视频本身,并且需要深度传感器.最近已经流行起来.[12]引入了深度和自我运动的联合学习.[50]中的zhou证明了一种完全可微的方法,其中深度和自我运动是通过深度神经网络联合预测的.単目和双目设置的技术在[ ... ]得到发展.结果表明当在训练过程中只使用深度输入时,推理深度质量得到提高,而不像其他方法那样依赖于推理时的立体视差[18,19,47] . 其他的新技术包括运动的运用[45,39,38,7,44]

##### 从原生的图片或者视频中学习

从原生的图像中学习深度也是一个积极的研究领域,大多集中于单视或者多视图图像,Li的实验表明由于输入源的多样性和对相机内参的未知,这对于互联网照片来说非常困难.通过从原生视频中学习内参,我们的工作为解决这个挑战迈出了一步.

##### 遮挡意识学习

已经提出了多种方法来处理光流环境中的遮挡,这些方法与几何无关,可微的网格渲染近年来收到越来越多的关注,开始对遮挡采用几何的方法.在学习预测深度和自我运动的背景下,通过已学习的可解释性掩码,通过惩罚前一帧或者后一帧到中间帧的最小重投影函数损失,并通过光流来解决遮挡问题.在视频无监督学习的背景下,通过一个可微损失,我们第一个提出一种直接使用几何方法来应对遮挡.

##### 内参学习

学习估计相机内参大多局限于强监督的方法,底层真相的来源多种多样:[41]中的workman使用的焦距估计采用经典的从运动中获取一维结构.[43]中的yan基于EXIF获得焦距.[4]中的Bogdan是使用已知内参的虚拟相机从全景图中合成图像,包括畸变.据我们所知,我们的方法是唯一一种直接从视频中,结合深度,自我运动和物体运动,以无监督的方式学习相机内参的方法.

#### 3预先准备

我们的方法拓展了同时学习深度和运动领域的现有技术[35 14 48 36 ].与之前的工作类似,我们方法的核心是使用深度图和相机矩阵将两个相邻的视频帧连接在一起的方程.

![1584764354470](/home/ykh/.config/Typora/typora-user-images/1584764354470.png)

其中K是内参矩阵

其中p和p'分别为旋转矩阵R和平移向量t所表示的变换前后的齐次像素坐标.z和z'是各自的深度.使用深度网络预测的z,R和t,等式(1)将一个视频帧投影到另一个上,结果与实际的其他帧比较,这些差异构成了训练损失的主要组成部分,前提是通过惩罚差异,网络将学会正确的预测z,R和t

#### 4 方法

在这项工作中,我们提出从単目视频中同时学习深度,自我运动,物体运动和相机内参.为了完成这项工作,我们设计了一个运动预测网络,它可以预测相机的运动,每个像素相对于背景的运动和相机内参:焦距,偏移和畸变.第二个网络预测深度地图,通过将相邻帧之间的一致性作为一种损失,网络同时学习预测深度图,运动范围和相机内参.我们引入了一种损失,只要求在两帧中未被遮挡的像素的一致性,在这种情况下,遮挡估计是通过所学的深度图本身进行几何估计实现的.运动区域在一个掩码的帮助下被正则化,该掩码从一个预先训练的分割或者物体检测网络中得到,表明了可能属于移动物体的像素.

##### 4.1 学习相机内参

在训练中,帧间一致性信号通过p'传回学习量K,R,z和t,因为等式1仅通过Kt和K-1 RK依赖于K,训练的损失会将这两个量驱动到正确的值,但是Kt可以是完全正确的,即使K和t是不正确的,事实上,如果网络预测了一个不正确的内参矩阵~K,和不正确的平移向量~t,~K~t仍然与Kt相等.所以训练损失不受影响.

尽管平移没有为K提供监督信号,幸运的是旋转可以,附录中的推导表明不存在~K,~R以至于~K~R~K-1 = KRK-1,等式3将可以从两帧中确定焦距的公差和两帧之间相机的旋转量联系起来.

![1584766393971](/home/ykh/.config/Typora/typora-user-images/1584766393971.png)

ry和rx是弧度制表示的绕轴旋转角度,w和h分别是图片的宽度和高度.

##### 4.2 学习物体运动

等式1可以将帧不一致性损失传播到每个像素的z,R和t,然而,没有进一步的正则化,后三者仍然存在很大的不确定性,尽管z,R和t的连续性是一个很强大的正则化器,我们发现进一步的正则化有很大帮助,特别的,我们在整个图像中施加了R的恒常性,并且只允许t在指定为"可能移动"的像素点偏离一个恒定值.这个掩码可以从一个预先训练的分割模型中获得.与之前的工作不同,实例分割和追踪是不需要的,因为我们需要一个的可能移动的掩码.事实上,我们证明了边框的集合是足够的,此外,对t应用L1平滑算子 

##### 4.3 occlusion-aware一致性

当摄像机相对于场景移动时,并且/或者物体移动,在一帧中可见的场景中的点在另一帧中可能被遮挡,反之亦然.在这些点对应的像素中,跨帧一致性不能通过损失来执行,给定一个深度图和一帧中的运动区域,我们可以检测出遮挡在哪里发生,白关切从一致性损失中排除遮挡区域.

虽然检测被遮挡的像素是可行的,但他需要对由深度图表示的表面的连续性和z缓冲进行某种推理.保持该机制的可微行和有效性,以适应一个训练循环,可能是一个挑战.

因此我们采用了不同的方法,就像图3中阐述的那样,对源帧中的每个像素(i,j),使用预测的深度zij和相机内参矩阵获得各自空间中的点(xij,yij,zij),这个点根据预测的运动范围在空间中被移动.特别的,深度变为z',新的空间位置被重投影到相机帧中,落在目标帧中通常情况下不同的一点(i',j').i'和j'通常情况下是非整数,因此在目标帧中点(i',j')处获取的深度zi'j't,需要插值

遮挡发生在(i',j')处,此时z'变为多值,在这些点上,颜色和深度一致性应该只应用在z'的可见分支上,就是说拥有更小z'值得分支.如果源帧和目标帧几乎一致,可见分支将接近点(i',j')的目标深度zi'j't,我们提出的检测方法是仅包括满足z'i'j' < zi'j't的点(i',j').换句话说,只有当源帧上的一个变换后的像素落在目标帧深度图前面时,我们才会将像素包括在损失中.该方案在源帧和目标帧的交换上是不对称的,这就是为什么我们总是以一种对称的方式来应用它:我们转换源帧到目标帧,计算损失,然后交换源帧和目标帧的角色.图3阐述了这个方法.这种应用损失的方法可以被很多类型的损失应用调用,在本文中我们称他为 "occlusion-aware"损失或惩罚

##### 4.4 网络,损失和正则化

**网络** 我们依赖于两个卷积神经网络,一个从单一图像中预测深度,一另一个从两张图像中预测 自我运动,相对于背景的物体运动领域和相机内参.深度预测网络是一个UNet结构,拥有个ResNet18 base 和softplus函数来讲logits转换为深度.

运动估计网络是一个UNet结构灵感来自于FlowNet[9],一个包含stride 2 (编码器) 的卷积栈,平均池是最后一个,形成了一个带有1*1空间分辨率的,1024通道的瓶颈.两个带有三通道的卷积分别预测全局旋转角度和全局平移向量.后两个代表了整个场景相对于摄像机的运动.每一个相机内参由一个瓶颈产生的1\*1卷积预测,softplus计算焦距和畸变,下一层逐步完善平移,从单一向量到残差平移矢量场.通过2倍的高度和尺寸.

在这里我们使用4.2中描述的前景掩码m(x,y).平移域表示为全局平移向量和掩码后的残差平移的总和.

![1584775428346](/home/ykh/.config/Typora/typora-user-images/1584775428346.png)

m(x,y) 在移动物体的像素点处是1,否则是0

##### 损失 

给定一对帧,我们对每一个RGB颜色通道和深度应用一个带有遮挡感知的L1惩罚,对于运动区域,我们要求循环一致性:源帧在像素(i, j)处的旋转和平移必须与目标帧在像素(i', j')处的旋转和平移形成相反的变化,反之亦然.这里也调用了遮挡感知.

结构相似度(SSIM)是训练损失的重要组成部分,而上面定义的遮挡感知在这里很难实施.因为SSIM涉及到每个像素的邻域.有可能z'<= zt仅仅在邻域像素的一部分上才成立.我们在这里找到的解决方案是通过一个函数来衡量结构相似度,当帧间的深度差异比深度差异的均方根值大时,这个函数就会失效.

##### 随机层归一化

一开始我们的深度预测网络是批量归一化,但是我们重复的发现他会导致异常的行为:

* 在批处理归一化的"训练模式"下运行推断时Eval 准确性始终更好.也就是说,不是长期的平均均值和方差,而是在推断过程中从图像本身得到的均值和方差被使用,使批处理归一化更加类似于图层归一化
* 当我们在训练中增加批处理的大小时,不管我们如何拓展学习率,eval  的准确率总是越来越差

这两个发现使我们尝试用图层归一化代替批处理归一化,但是eval的质量却下降啦.我们的下一个假设是虽然批处理归一化实际上像图层归一化一样,但批处理的其他项在此基础上作为噪声的来源.为了验证这一理论,我们用 在层均值和方差上添加高斯噪声的层归一化代替批处理归一化.

确实,与批处理归一化相比,所有的eval指标都有了显著地提升.除此之外,当训练的批处理大小增加时,eval指标开始略有改善,利用乘性噪声获得了最佳结果.虽然在过去已经观察到噪声可以作为一个正则化器,例如用dropout[34]或应用到梯度时,我们并不知道在之前的工作中,它是用层归一化.

