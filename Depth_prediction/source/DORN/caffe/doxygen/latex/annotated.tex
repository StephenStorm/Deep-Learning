\section{Class List}
Here are the classes, structs, unions and interfaces with brief descriptions\+:\begin{DoxyCompactList}
\item\contentsline{section}{\hyperlink{classcaffe_1_1AbsValLayer}{caffe\+::\+Abs\+Val\+Layer$<$ Dtype $>$} \\*Computes $ y = |x| $ }{\pageref{classcaffe_1_1AbsValLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1AccuracyLayer}{caffe\+::\+Accuracy\+Layer$<$ Dtype $>$} \\*Computes the classification accuracy for a one-\/of-\/many classification task }{\pageref{classcaffe_1_1AccuracyLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1AdaDeltaSolver}{caffe\+::\+Ada\+Delta\+Solver$<$ Dtype $>$} }{\pageref{classcaffe_1_1AdaDeltaSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1AdaGradSolver}{caffe\+::\+Ada\+Grad\+Solver$<$ Dtype $>$} }{\pageref{classcaffe_1_1AdaGradSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1AdamSolver}{caffe\+::\+Adam\+Solver$<$ Dtype $>$} \\*\hyperlink{classcaffe_1_1AdamSolver}{Adam\+Solver}, an algorithm for first-\/order gradient-\/based optimization of stochastic objective functions, based on adaptive estimates of lower-\/order moments. Described in \mbox{[}1\mbox{]} }{\pageref{classcaffe_1_1AdamSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ArgMaxLayer}{caffe\+::\+Arg\+Max\+Layer$<$ Dtype $>$} \\*Compute the index of the $ K $ max values for each datum across all dimensions $ (C \times H \times W) $ }{\pageref{classcaffe_1_1ArgMaxLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BaseConvolutionLayer}{caffe\+::\+Base\+Convolution\+Layer$<$ Dtype $>$} \\*Abstract base class that factors out the B\+L\+AS code common to \hyperlink{classcaffe_1_1ConvolutionLayer}{Convolution\+Layer} and \hyperlink{classcaffe_1_1DeconvolutionLayer}{Deconvolution\+Layer} }{\pageref{classcaffe_1_1BaseConvolutionLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BaseDataLayer}{caffe\+::\+Base\+Data\+Layer$<$ Dtype $>$} \\*Provides base for data layers that feed blobs to the \hyperlink{classcaffe_1_1Net}{Net} }{\pageref{classcaffe_1_1BaseDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BasePrefetchingDataLayer}{caffe\+::\+Base\+Prefetching\+Data\+Layer$<$ Dtype $>$} }{\pageref{classcaffe_1_1BasePrefetchingDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Batch}{caffe\+::\+Batch$<$ Dtype $>$} }{\pageref{classcaffe_1_1Batch}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BatchNormLayer}{caffe\+::\+Batch\+Norm\+Layer$<$ Dtype $>$} \\*Normalizes the input to have 0-\/mean and/or unit (1) variance across the batch }{\pageref{classcaffe_1_1BatchNormLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BatchReindexLayer}{caffe\+::\+Batch\+Reindex\+Layer$<$ Dtype $>$} \\*Index into the input blob along its first axis }{\pageref{classcaffe_1_1BatchReindexLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BiasLayer}{caffe\+::\+Bias\+Layer$<$ Dtype $>$} \\*Computes a sum of two input Blobs, with the shape of the latter \hyperlink{classcaffe_1_1Blob}{Blob} \char`\"{}broadcast\char`\"{} to match the shape of the former. Equivalent to tiling the latter \hyperlink{classcaffe_1_1Blob}{Blob}, then computing the elementwise sum }{\pageref{classcaffe_1_1BiasLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BilinearFiller}{caffe\+::\+Bilinear\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with coefficients for bilinear interpolation }{\pageref{classcaffe_1_1BilinearFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Blob}{caffe\+::\+Blob$<$ Dtype $>$} \\*A wrapper around \hyperlink{classcaffe_1_1SyncedMemory}{Synced\+Memory} holders serving as the basic computational unit through which \hyperlink{classcaffe_1_1Layer}{Layer}s, \hyperlink{classcaffe_1_1Net}{Net}s, and \hyperlink{classcaffe_1_1Solver}{Solver}s interact }{\pageref{classcaffe_1_1Blob}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BlockingQueue}{caffe\+::\+Blocking\+Queue$<$ T $>$} }{\pageref{classcaffe_1_1BlockingQueue}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BNLayer}{caffe\+::\+B\+N\+Layer$<$ Dtype $>$} \\*\hyperlink{classcaffe_1_1Batch}{Batch} normalization the input blob along the channel axis while averaging over the spatial axes }{\pageref{classcaffe_1_1BNLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BNLLLayer}{caffe\+::\+B\+N\+L\+L\+Layer$<$ Dtype $>$} \\*Computes $ y = x + \log(1 + \exp(-x)) $ if $ x > 0 $; $ y = \log(1 + \exp(x)) $ otherwise }{\pageref{classcaffe_1_1BNLLLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Caffe}{caffe\+::\+Caffe} }{\pageref{classcaffe_1_1Caffe}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Net_1_1Callback}{caffe\+::\+Net$<$ Dtype $>$\+::\+Callback} }{\pageref{classcaffe_1_1Net_1_1Callback}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Solver_1_1Callback}{caffe\+::\+Solver$<$ Dtype $>$\+::\+Callback} }{\pageref{classcaffe_1_1Solver_1_1Callback}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ConcatLayer}{caffe\+::\+Concat\+Layer$<$ Dtype $>$} \\*Takes at least two \hyperlink{classcaffe_1_1Blob}{Blob}s and concatenates them along either the num or channel dimension, outputting the result }{\pageref{classcaffe_1_1ConcatLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ConstantFiller}{caffe\+::\+Constant\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with constant values $ x = 0 $ }{\pageref{classcaffe_1_1ConstantFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ContrastiveLossLayer}{caffe\+::\+Contrastive\+Loss\+Layer$<$ Dtype $>$} \\*Computes the contrastive loss $ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $ where $ d = \left| \left| a_n - b_n \right| \right|_2 $. This can be used to train siamese networks }{\pageref{classcaffe_1_1ContrastiveLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ConvolutionLayer}{caffe\+::\+Convolution\+Layer$<$ Dtype $>$} \\*Convolves the input image with a bank of learned filters, and (optionally) adds biases }{\pageref{classcaffe_1_1ConvolutionLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1CPUTimer}{caffe\+::\+C\+P\+U\+Timer} }{\pageref{classcaffe_1_1CPUTimer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1CropLayer}{caffe\+::\+Crop\+Layer$<$ Dtype $>$} \\*Takes a \hyperlink{classcaffe_1_1Blob}{Blob} and crop it, to the shape specified by the second input \hyperlink{classcaffe_1_1Blob}{Blob}, across all dimensions after the specified axis }{\pageref{classcaffe_1_1CropLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1db_1_1Cursor}{caffe\+::db\+::\+Cursor} }{\pageref{classcaffe_1_1db_1_1Cursor}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1DataLayer}{caffe\+::\+Data\+Layer$<$ Dtype $>$} }{\pageref{classcaffe_1_1DataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1DataTransformer}{caffe\+::\+Data\+Transformer$<$ Dtype $>$} \\*Applies common transformations to the input data, such as scaling, mirroring, substracting the image mean.. }{\pageref{classcaffe_1_1DataTransformer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1db_1_1DB}{caffe\+::db\+::\+DB} }{\pageref{classcaffe_1_1db_1_1DB}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1DeconvolutionLayer}{caffe\+::\+Deconvolution\+Layer$<$ Dtype $>$} \\*Convolve the input with a bank of learned filters, and (optionally) add biases, treating filters and convolution parameters in the opposite sense as \hyperlink{classcaffe_1_1ConvolutionLayer}{Convolution\+Layer} }{\pageref{classcaffe_1_1DeconvolutionLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1DropoutLayer}{caffe\+::\+Dropout\+Layer$<$ Dtype $>$} \\*During training only, sets a random portion of $x$ to 0, adjusting the rest of the vector magnitude accordingly }{\pageref{classcaffe_1_1DropoutLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1DummyDataLayer}{caffe\+::\+Dummy\+Data\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} generated by a \hyperlink{classcaffe_1_1Filler}{Filler} }{\pageref{classcaffe_1_1DummyDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1EltwiseLayer}{caffe\+::\+Eltwise\+Layer$<$ Dtype $>$} \\*Compute elementwise operations, such as product and sum, along multiple input Blobs }{\pageref{classcaffe_1_1EltwiseLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ELULayer}{caffe\+::\+E\+L\+U\+Layer$<$ Dtype $>$} \\*Exponential Linear Unit non-\/linearity $ y = \left\{ \begin{array}{lr} x & \mathrm{if} \; x > 0 \\ \alpha (\exp(x)-1) & \mathrm{if} \; x \le 0 \end{array} \right. $ }{\pageref{classcaffe_1_1ELULayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1EmbedLayer}{caffe\+::\+Embed\+Layer$<$ Dtype $>$} \\*A layer for learning \char`\"{}embeddings\char`\"{} of one-\/hot vector input. Equivalent to an \hyperlink{classcaffe_1_1InnerProductLayer}{Inner\+Product\+Layer} with one-\/hot vectors as input, but for efficiency the input is the \char`\"{}hot\char`\"{} index of each column itself }{\pageref{classcaffe_1_1EmbedLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1EuclideanLossLayer}{caffe\+::\+Euclidean\+Loss\+Layer$<$ Dtype $>$} \\*Computes the Euclidean (L2) loss $ E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $ for real-\/valued regression tasks }{\pageref{classcaffe_1_1EuclideanLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ExpLayer}{caffe\+::\+Exp\+Layer$<$ Dtype $>$} \\*Computes $ y = \gamma ^ {\alpha x + \beta} $, as specified by the scale $ \alpha $, shift $ \beta $, and base $ \gamma $ }{\pageref{classcaffe_1_1ExpLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Filler}{caffe\+::\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with constant or randomly-\/generated data }{\pageref{classcaffe_1_1Filler}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1FilterLayer}{caffe\+::\+Filter\+Layer$<$ Dtype $>$} \\*Takes two+ Blobs, interprets last \hyperlink{classcaffe_1_1Blob}{Blob} as a selector and filter remaining Blobs accordingly with selector data (0 means that the corresponding item has to be filtered, non-\/zero means that corresponding item needs to stay) }{\pageref{classcaffe_1_1FilterLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1FlattenLayer}{caffe\+::\+Flatten\+Layer$<$ Dtype $>$} \\*Reshapes the input \hyperlink{classcaffe_1_1Blob}{Blob} into flat vectors }{\pageref{classcaffe_1_1FlattenLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1GaussianFiller}{caffe\+::\+Gaussian\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with Gaussian-\/distributed values $ x = a $ }{\pageref{classcaffe_1_1GaussianFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Caffe_1_1RNG_1_1Generator}{caffe\+::\+Caffe\+::\+R\+N\+G\+::\+Generator} }{\pageref{classcaffe_1_1Caffe_1_1RNG_1_1Generator}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1HDF5DataLayer}{caffe\+::\+H\+D\+F5\+Data\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} from H\+D\+F5 files }{\pageref{classcaffe_1_1HDF5DataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1HDF5OutputLayer}{caffe\+::\+H\+D\+F5\+Output\+Layer$<$ Dtype $>$} \\*Write blobs to disk as H\+D\+F5 files }{\pageref{classcaffe_1_1HDF5OutputLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1HingeLossLayer}{caffe\+::\+Hinge\+Loss\+Layer$<$ Dtype $>$} \\*Computes the hinge loss for a one-\/of-\/many classification task }{\pageref{classcaffe_1_1HingeLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Im2colLayer}{caffe\+::\+Im2col\+Layer$<$ Dtype $>$} \\*A helper for image operations that rearranges image regions into column vectors. Used by \hyperlink{classcaffe_1_1ConvolutionLayer}{Convolution\+Layer} to perform convolution by matrix multiplication }{\pageref{classcaffe_1_1Im2colLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ImageDataLayer}{caffe\+::\+Image\+Data\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} from image files }{\pageref{classcaffe_1_1ImageDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1InfogainLossLayer}{caffe\+::\+Infogain\+Loss\+Layer$<$ Dtype $>$} \\*A generalization of \hyperlink{classcaffe_1_1SoftmaxWithLossLayer}{Softmax\+With\+Loss\+Layer} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs }{\pageref{classcaffe_1_1InfogainLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1InnerProductLayer}{caffe\+::\+Inner\+Product\+Layer$<$ Dtype $>$} \\*Also known as a \char`\"{}fully-\/connected\char`\"{} layer, computes an inner product with a set of learned weights, and (optionally) adds biases }{\pageref{classcaffe_1_1InnerProductLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1InputLayer}{caffe\+::\+Input\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} by assigning tops directly }{\pageref{classcaffe_1_1InputLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1InternalThread}{caffe\+::\+Internal\+Thread} }{\pageref{classcaffe_1_1InternalThread}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1InterpLayer}{caffe\+::\+Interp\+Layer$<$ Dtype $>$} \\*Changes the spatial resolution by bi-\/linear interpolation. The target size is specified in terms of pixels. The start and end pixels of the input are mapped to the start and end pixels of the output }{\pageref{classcaffe_1_1InterpLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Layer}{caffe\+::\+Layer$<$ Dtype $>$} \\*An interface for the units of computation which can be composed into a \hyperlink{classcaffe_1_1Net}{Net} }{\pageref{classcaffe_1_1Layer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LayerRegisterer}{caffe\+::\+Layer\+Registerer$<$ Dtype $>$} }{\pageref{classcaffe_1_1LayerRegisterer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LayerRegistry}{caffe\+::\+Layer\+Registry$<$ Dtype $>$} }{\pageref{classcaffe_1_1LayerRegistry}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LogLayer}{caffe\+::\+Log\+Layer$<$ Dtype $>$} \\*Computes $ y = log_{\gamma}(\alpha x + \beta) $, as specified by the scale $ \alpha $, shift $ \beta $, and base $ \gamma $ }{\pageref{classcaffe_1_1LogLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LossLayer}{caffe\+::\+Loss\+Layer$<$ Dtype $>$} \\*An interface for \hyperlink{classcaffe_1_1Layer}{Layer}s that take two \hyperlink{classcaffe_1_1Blob}{Blob}s as input -- usually (1) predictions and (2) ground-\/truth labels -- and output a singleton \hyperlink{classcaffe_1_1Blob}{Blob} representing the loss }{\pageref{classcaffe_1_1LossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LRNLayer}{caffe\+::\+L\+R\+N\+Layer$<$ Dtype $>$} \\*Normalize the input in a local region across or within feature maps }{\pageref{classcaffe_1_1LRNLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LSTMLayer}{caffe\+::\+L\+S\+T\+M\+Layer$<$ Dtype $>$} \\*Processes sequential inputs using a \char`\"{}\+Long Short-\/\+Term Memory\char`\"{} (L\+S\+TM) \mbox{[}1\mbox{]} style recurrent neural network (R\+NN). Implemented by unrolling the L\+S\+TM computation through time }{\pageref{classcaffe_1_1LSTMLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1LSTMUnitLayer}{caffe\+::\+L\+S\+T\+M\+Unit\+Layer$<$ Dtype $>$} \\*A helper for \hyperlink{classcaffe_1_1LSTMLayer}{L\+S\+T\+M\+Layer}\+: computes a single timestep of the non-\/linearity of the L\+S\+TM, producing the updated cell and hidden states }{\pageref{classcaffe_1_1LSTMUnitLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1MemoryDataLayer}{caffe\+::\+Memory\+Data\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} from memory }{\pageref{classcaffe_1_1MemoryDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1MSRAFiller}{caffe\+::\+M\+S\+R\+A\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with values $ x \sim N(0, \sigma^2) $ where $ \sigma^2 $ is set inversely proportional to number of incoming nodes, outgoing nodes, or their average }{\pageref{classcaffe_1_1MSRAFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1MultinomialLogisticLossLayer}{caffe\+::\+Multinomial\+Logistic\+Loss\+Layer$<$ Dtype $>$} \\*Computes the multinomial logistic loss for a one-\/of-\/many classification task, directly taking a predicted probability distribution as input }{\pageref{classcaffe_1_1MultinomialLogisticLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1MVNLayer}{caffe\+::\+M\+V\+N\+Layer$<$ Dtype $>$} \\*Normalizes the input to have 0-\/mean and/or unit (1) variance }{\pageref{classcaffe_1_1MVNLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1NesterovSolver}{caffe\+::\+Nesterov\+Solver$<$ Dtype $>$} }{\pageref{classcaffe_1_1NesterovSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Net}{caffe\+::\+Net$<$ Dtype $>$} \\*Connects \hyperlink{classcaffe_1_1Layer}{Layer}s together into a directed acyclic graph (D\+AG) specified by a Net\+Parameter }{\pageref{classcaffe_1_1Net}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1NeuronLayer}{caffe\+::\+Neuron\+Layer$<$ Dtype $>$} \\*An interface for layers that take one blob as input ( $ x $) and produce one equally-\/sized blob as output ( $ y $), where each element of the output depends only on the corresponding input element }{\pageref{classcaffe_1_1NeuronLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ParameterLayer}{caffe\+::\+Parameter\+Layer$<$ Dtype $>$} }{\pageref{classcaffe_1_1ParameterLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1PoolingLayer}{caffe\+::\+Pooling\+Layer$<$ Dtype $>$} \\*Pools the input image by taking the max, average, etc. within regions }{\pageref{classcaffe_1_1PoolingLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1PositiveUnitballFiller}{caffe\+::\+Positive\+Unitball\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with values $ x \in [0, 1] $ such that $ \forall i \sum_j x_{ij} = 1 $ }{\pageref{classcaffe_1_1PositiveUnitballFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1PowerLayer}{caffe\+::\+Power\+Layer$<$ Dtype $>$} \\*Computes $ y = (\alpha x + \beta) ^ \gamma $, as specified by the scale $ \alpha $, shift $ \beta $, and power $ \gamma $ }{\pageref{classcaffe_1_1PowerLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1PReLULayer}{caffe\+::\+P\+Re\+L\+U\+Layer$<$ Dtype $>$} \\*Parameterized Rectified Linear Unit non-\/linearity $ y_i = \max(0, x_i) + a_i \min(0, x_i) $. The differences from \hyperlink{classcaffe_1_1ReLULayer}{Re\+L\+U\+Layer} are 1) negative slopes are learnable though backprop and 2) negative slopes can vary across channels. The number of axes of input blob should be greater than or equal to 2. The 1st axis (0-\/based) is seen as channels }{\pageref{classcaffe_1_1PReLULayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1PythonLayer}{caffe\+::\+Python\+Layer$<$ Dtype $>$} }{\pageref{classcaffe_1_1PythonLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1RecurrentLayer}{caffe\+::\+Recurrent\+Layer$<$ Dtype $>$} \\*An abstract class for implementing recurrent behavior inside of an unrolled network. This \hyperlink{classcaffe_1_1Layer}{Layer} type cannot be instantiated -- instead, you should use one of its implementations which defines the recurrent architecture, such as \hyperlink{classcaffe_1_1RNNLayer}{R\+N\+N\+Layer} or \hyperlink{classcaffe_1_1LSTMLayer}{L\+S\+T\+M\+Layer} }{\pageref{classcaffe_1_1RecurrentLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ReductionLayer}{caffe\+::\+Reduction\+Layer$<$ Dtype $>$} \\*Compute \char`\"{}reductions\char`\"{} -- operations that return a scalar output \hyperlink{classcaffe_1_1Blob}{Blob} for an input \hyperlink{classcaffe_1_1Blob}{Blob} of arbitrary size, such as the sum, absolute sum, and sum of squares }{\pageref{classcaffe_1_1ReductionLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ReLULayer}{caffe\+::\+Re\+L\+U\+Layer$<$ Dtype $>$} \\*Rectified Linear Unit non-\/linearity $ y = \max(0, x) $. The simple max is fast to compute, and the function does not saturate }{\pageref{classcaffe_1_1ReLULayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ReshapeLayer}{caffe\+::\+Reshape\+Layer$<$ Dtype $>$} }{\pageref{classcaffe_1_1ReshapeLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1RMSPropSolver}{caffe\+::\+R\+M\+S\+Prop\+Solver$<$ Dtype $>$} }{\pageref{classcaffe_1_1RMSPropSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Caffe_1_1RNG}{caffe\+::\+Caffe\+::\+R\+NG} }{\pageref{classcaffe_1_1Caffe_1_1RNG}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1RNNLayer}{caffe\+::\+R\+N\+N\+Layer$<$ Dtype $>$} \\*Processes time-\/varying inputs using a simple recurrent neural network (R\+NN). Implemented as a network unrolling the R\+NN computation in time }{\pageref{classcaffe_1_1RNNLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ScaleLayer}{caffe\+::\+Scale\+Layer$<$ Dtype $>$} \\*Computes the elementwise product of two input Blobs, with the shape of the latter \hyperlink{classcaffe_1_1Blob}{Blob} \char`\"{}broadcast\char`\"{} to match the shape of the former. Equivalent to tiling the latter \hyperlink{classcaffe_1_1Blob}{Blob}, then computing the elementwise product. Note\+: for efficiency and convenience, this layer can additionally perform a \char`\"{}broadcast\char`\"{} sum too when {\ttfamily bias\+\_\+term\+: true} is set }{\pageref{classcaffe_1_1ScaleLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SGDSolver}{caffe\+::\+S\+G\+D\+Solver$<$ Dtype $>$} \\*Optimizes the parameters of a \hyperlink{classcaffe_1_1Net}{Net} using stochastic gradient descent (S\+GD) with momentum }{\pageref{classcaffe_1_1SGDSolver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SigmoidCrossEntropyLossLayer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$} \\*Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities }{\pageref{classcaffe_1_1SigmoidCrossEntropyLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SigmoidLayer}{caffe\+::\+Sigmoid\+Layer$<$ Dtype $>$} \\*Sigmoid function non-\/linearity $ y = (1 + \exp(-x))^{-1} $, a classic choice in neural networks }{\pageref{classcaffe_1_1SigmoidLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SignalHandler}{caffe\+::\+Signal\+Handler} }{\pageref{classcaffe_1_1SignalHandler}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SilenceLayer}{caffe\+::\+Silence\+Layer$<$ Dtype $>$} \\*Ignores bottom blobs while producing no top blobs. (This is useful to suppress outputs during testing.) }{\pageref{classcaffe_1_1SilenceLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SliceLayer}{caffe\+::\+Slice\+Layer$<$ Dtype $>$} \\*Takes a \hyperlink{classcaffe_1_1Blob}{Blob} and slices it along either the num or channel dimension, outputting multiple sliced \hyperlink{classcaffe_1_1Blob}{Blob} results }{\pageref{classcaffe_1_1SliceLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SoftmaxLayer}{caffe\+::\+Softmax\+Layer$<$ Dtype $>$} \\*Computes the softmax function }{\pageref{classcaffe_1_1SoftmaxLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SoftmaxWithLossLayer}{caffe\+::\+Softmax\+With\+Loss\+Layer$<$ Dtype $>$} \\*Computes the multinomial logistic loss for a one-\/of-\/many classification task, passing real-\/valued predictions through a softmax to get a probability distribution over classes }{\pageref{classcaffe_1_1SoftmaxWithLossLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Solver}{caffe\+::\+Solver$<$ Dtype $>$} \\*An interface for classes that perform optimization on \hyperlink{classcaffe_1_1Net}{Net}s }{\pageref{classcaffe_1_1Solver}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SolverRegisterer}{caffe\+::\+Solver\+Registerer$<$ Dtype $>$} }{\pageref{classcaffe_1_1SolverRegisterer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SolverRegistry}{caffe\+::\+Solver\+Registry$<$ Dtype $>$} }{\pageref{classcaffe_1_1SolverRegistry}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SplitLayer}{caffe\+::\+Split\+Layer$<$ Dtype $>$} \\*Creates a \char`\"{}split\char`\"{} path in the network by copying the bottom \hyperlink{classcaffe_1_1Blob}{Blob} into multiple top \hyperlink{classcaffe_1_1Blob}{Blob}s to be used by multiple consuming layers }{\pageref{classcaffe_1_1SplitLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SPPLayer}{caffe\+::\+S\+P\+P\+Layer$<$ Dtype $>$} \\*Does spatial pyramid pooling on the input image by taking the max, average, etc. within regions so that the result vector of different sized images are of the same size }{\pageref{classcaffe_1_1SPPLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SwishLayer}{caffe\+::\+Swish\+Layer$<$ Dtype $>$} \\*Swish non-\/linearity $ y = x \sigma (\beta x) $. A novel activation function that tends to work better than Re\+LU \mbox{[}1\mbox{]} }{\pageref{classcaffe_1_1SwishLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1BlockingQueue_1_1sync}{caffe\+::\+Blocking\+Queue$<$ T $>$\+::sync} }{\pageref{classcaffe_1_1BlockingQueue_1_1sync}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1SyncedMemory}{caffe\+::\+Synced\+Memory} \\*Manages memory allocation and synchronization between the host (C\+PU) and device (G\+PU) }{\pageref{classcaffe_1_1SyncedMemory}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1TanHLayer}{caffe\+::\+Tan\+H\+Layer$<$ Dtype $>$} \\*TanH hyperbolic tangent non-\/linearity $ y = \frac{\exp(2x) - 1}{\exp(2x) + 1} $, popular in auto-\/encoders }{\pageref{classcaffe_1_1TanHLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1ThresholdLayer}{caffe\+::\+Threshold\+Layer$<$ Dtype $>$} \\*Tests whether the input exceeds a threshold\+: outputs 1 for inputs above threshold; 0 otherwise }{\pageref{classcaffe_1_1ThresholdLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1TileLayer}{caffe\+::\+Tile\+Layer$<$ Dtype $>$} \\*Copy a \hyperlink{classcaffe_1_1Blob}{Blob} along specified dimensions }{\pageref{classcaffe_1_1TileLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1Timer}{caffe\+::\+Timer} }{\pageref{classcaffe_1_1Timer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1db_1_1Transaction}{caffe\+::db\+::\+Transaction} }{\pageref{classcaffe_1_1db_1_1Transaction}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1UniformFiller}{caffe\+::\+Uniform\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with uniformly distributed values $ x\sim U(a, b) $ }{\pageref{classcaffe_1_1UniformFiller}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1WindowDataLayer}{caffe\+::\+Window\+Data\+Layer$<$ Dtype $>$} \\*Provides data to the \hyperlink{classcaffe_1_1Net}{Net} from windows of images files, specified by a window data file. This layer is {\itshape D\+E\+P\+R\+E\+C\+A\+T\+ED} and only kept for archival purposes for use by the original R-\/\+C\+NN }{\pageref{classcaffe_1_1WindowDataLayer}}{}
\item\contentsline{section}{\hyperlink{classcaffe_1_1XavierFiller}{caffe\+::\+Xavier\+Filler$<$ Dtype $>$} \\*Fills a \hyperlink{classcaffe_1_1Blob}{Blob} with values $ x \sim U(-a, +a) $ where $ a $ is set inversely proportional to number of incoming nodes, outgoing nodes, or their average }{\pageref{classcaffe_1_1XavierFiller}}{}
\end{DoxyCompactList}
